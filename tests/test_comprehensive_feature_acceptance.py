"""Comprehensive Feature Acceptance Test for RAG Scraper.

This test validates all current features including:
- Single-page scraping with progress tracking
- Multi-page scraping with enhanced progress visualization
- Batch processing capabilities
- Advanced configuration options
- Real-time progress monitoring
- Enhanced results display
- File generation and download
- Error handling and recovery
"""

import pytest
import requests
import json
import time
from typing import Dict, Any, List


class TestComprehensiveFeatures:
    """Comprehensive acceptance tests for RAG Scraper features."""
    
    BASE_URL = "http://localhost:8085"
    TEST_URL = "https://mettavern.com/"
    
    @pytest.fixture(autouse=True)
    def setup_test(self):
        """Set up test environment and verify server is running."""
        try:
            response = requests.get(f"{self.BASE_URL}/", timeout=10)
            assert response.status_code == 200, f"Server not accessible: {response.status_code}"
        except requests.exceptions.RequestException as e:
            pytest.skip(f"RAG Scraper server not running: {e}")
    
    def test_single_page_scraping_with_progress(self):
        """Test single-page scraping with basic progress tracking."""
        print("\nüîç Testing Single-Page Scraping with Progress Tracking")
        
        # Configure single-page scraping
        payload = {
            "urls": [self.TEST_URL],
            "scraping_mode": "single",
            "file_format": "text",
            "output_dir": "/tmp",
            "file_mode": "single"
        }
        
        # Start scraping
        response = requests.post(f"{self.BASE_URL}/api/scrape", json=payload, timeout=60)
        
        assert response.status_code == 200, f"Scraping failed: {response.text}"
        
        data = response.json()
        assert data.get("success") is True, f"Scraping not successful: {data}"
        
        print(f"‚úÖ Single-page scraping completed successfully")
        print(f"   Processed: {data.get('processed_count', 0)} URLs")
        print(f"   Failed: {data.get('failed_count', 0)} URLs")
        print(f"   Processing time: {data.get('processing_time', 0):.2f}s")
        
        # Verify progress tracking capability
        progress_response = requests.get(f"{self.BASE_URL}/api/progress")
        if progress_response.status_code == 200:
            progress_data = progress_response.json()
            print(f"‚úÖ Progress API accessible")
            print(f"   Status: {progress_data.get('status', 'unknown')}")
        
        return data
    
    def test_multi_page_scraping_with_enhanced_progress(self):
        """Test multi-page scraping with enhanced progress visualization."""
        print("\nüåê Testing Multi-Page Scraping with Enhanced Progress")
        
        # Configure multi-page scraping
        payload = {
            "urls": [self.TEST_URL],
            "scraping_mode": "multi",
            "file_format": "text",
            "output_dir": "/tmp",
            "file_mode": "single",
            "multi_page_config": {
                "maxPages": 10,
                "crawlDepth": 2,
                "rateLimit": 1000,
                "includePatterns": "menu,food,restaurant,about,contact",
                "excludePatterns": "admin,login,cart,checkout"
            }
        }
        
        # Start multi-page scraping
        response = requests.post(f"{self.BASE_URL}/api/scrape", json=payload, timeout=120)
        
        assert response.status_code == 200, f"Multi-page scraping failed: {response.text}"
        
        data = response.json()
        assert data.get("success") is True, f"Multi-page scraping not successful: {data}"
        
        print(f"‚úÖ Multi-page scraping completed successfully")
        print(f"   Processed: {data.get('processed_count', 0)} extractions")
        print(f"   Failed: {data.get('failed_count', 0)} URLs")
        print(f"   Processing time: {data.get('processing_time', 0):.2f}s")
        
        # Verify enhanced progress features
        sites_data = data.get("sites_data", [])
        if sites_data:
            print(f"‚úÖ Enhanced results display available")
            for i, site in enumerate(sites_data):
                pages_processed = site.get("pages_processed", 0)
                site_url = site.get("site_url", "Unknown")
                print(f"   Site {i+1}: {site_url}")
                print(f"   Pages processed: {pages_processed}")
                
                # Verify page-level details
                pages = site.get("pages", [])
                if pages:
                    print(f"   Page details available: {len(pages)} pages")
                    for j, page in enumerate(pages[:3]):  # Show first 3 pages
                        status = page.get("status", "unknown")
                        processing_time = page.get("processing_time", 0)
                        page_url = page.get("url", "Unknown")
                        print(f"     Page {j+1}: {status} ({processing_time:.1f}s) - {page_url}")
        
        # Verify multi-page processing occurred
        total_pages = sum(site.get("pages_processed", 0) for site in sites_data)
        print(f"‚úÖ Total pages processed across all sites: {total_pages}")
        
        return data
    
    def test_batch_processing_multiple_urls(self):
        """Test batch processing with multiple URLs."""
        print("\nüì¶ Testing Batch Processing with Multiple URLs")
        
        # Configure batch processing
        test_urls = [
            self.TEST_URL,
            "https://example.com/",  # This might fail, testing error handling
        ]
        
        payload = {
            "urls": test_urls,
            "scraping_mode": "single",
            "file_format": "text",
            "output_dir": "/tmp",
            "file_mode": "single"
        }
        
        # Start batch processing
        response = requests.post(f"{self.BASE_URL}/api/scrape", json=payload, timeout=90)
        
        assert response.status_code == 200, f"Batch processing failed: {response.text}"
        
        data = response.json()
        print(f"‚úÖ Batch processing completed")
        print(f"   URLs submitted: {len(test_urls)}")
        print(f"   Successful extractions: {data.get('processed_count', 0)}")
        print(f"   Failed URLs: {data.get('failed_count', 0)}")
        
        # Verify individual URL tracking
        sites_data = data.get("sites_data", [])
        if sites_data:
            print(f"‚úÖ Individual URL tracking available")
            for site in sites_data:
                site_url = site.get("site_url", "Unknown")
                pages = site.get("pages", [])
                if pages:
                    status = pages[0].get("status", "unknown")
                    print(f"   {site_url}: {status}")
        
        return data
    
    def test_progress_api_real_time_features(self):
        """Test real-time progress API features."""
        print("\n‚è±Ô∏è Testing Real-Time Progress API Features")
        
        # Check progress API structure
        progress_response = requests.get(f"{self.BASE_URL}/api/progress")
        assert progress_response.status_code == 200, "Progress API not accessible"
        
        progress_data = progress_response.json()
        
        # Verify expected progress fields
        expected_fields = [
            "current_url", "urls_completed", "urls_total", "progress_percentage",
            "estimated_time_remaining", "current_operation", "memory_usage_mb",
            "status", "session_id", "progress_bar_percentage", "status_message"
        ]
        
        print(f"‚úÖ Progress API accessible")
        for field in expected_fields:
            if field in progress_data:
                value = progress_data[field]
                print(f"   {field}: {value}")
            else:
                print(f"   {field}: (not available)")
        
        # Verify progress visualization fields
        visualization_fields = [
            "page_progress", "notifications", "error_notifications", "completion_events"
        ]
        
        print(f"‚úÖ Progress visualization fields:")
        for field in visualization_fields:
            if field in progress_data:
                value = progress_data[field]
                if isinstance(value, list):
                    print(f"   {field}: {len(value)} items")
                else:
                    print(f"   {field}: {value}")
            else:
                print(f"   {field}: (not available)")
        
        return progress_data
    
    def test_file_generation_and_download(self):
        """Test file generation and download capabilities."""
        print("\nüìÑ Testing File Generation and Download")
        
        # First, perform a scraping operation to get data
        payload = {
            "urls": [self.TEST_URL],
            "scraping_mode": "single",
            "file_format": "text",
            "output_dir": "/tmp",
            "file_mode": "single"
        }
        
        response = requests.post(f"{self.BASE_URL}/api/scrape", json=payload, timeout=60)
        assert response.status_code == 200, "Initial scraping failed"
        
        data = response.json()
        output_files = data.get("output_files", [])
        
        print(f"‚úÖ File generation completed")
        print(f"   Generated files: {len(output_files)}")
        
        for file_path in output_files:
            print(f"   File: {file_path}")
        
        # Test file configuration API
        config_response = requests.get(f"{self.BASE_URL}/api/file-config")
        if config_response.status_code == 200:
            config_data = config_response.json()
            print(f"‚úÖ File configuration API accessible")
            print(f"   Supported formats: {config_data.get('supported_formats', [])}")
        
        return data
    
    def test_error_handling_and_recovery(self):
        """Test error handling and recovery mechanisms."""
        print("\nüö® Testing Error Handling and Recovery")
        
        # Test with invalid URL
        invalid_payload = {
            "urls": ["https://this-domain-definitely-does-not-exist-12345.invalid/"],
            "scraping_mode": "single",
            "file_format": "text",
            "output_dir": "/tmp",
            "file_mode": "single"
        }
        
        response = requests.post(f"{self.BASE_URL}/api/scrape", json=invalid_payload, timeout=30)
        
        # The API should handle this gracefully
        if response.status_code == 200:
            data = response.json()
            print(f"‚úÖ Invalid URL handled gracefully")
            print(f"   Success: {data.get('success')}")
            print(f"   Failed count: {data.get('failed_count', 0)}")
            
            # Should show the URL as failed
            assert data.get("failed_count", 0) >= 0, "Failed URL count not tracked"
            
        elif response.status_code == 400:
            error_data = response.json()
            print(f"‚úÖ Invalid URL properly rejected")
            print(f"   Error: {error_data.get('error', 'Unknown error')}")
        
        # Test mixed valid/invalid URLs
        mixed_payload = {
            "urls": [
                self.TEST_URL,  # Valid
                "https://invalid-url-12345.invalid/"  # Invalid
            ],
            "scraping_mode": "single",
            "file_format": "text",
            "output_dir": "/tmp",
            "file_mode": "single"
        }
        
        mixed_response = requests.post(f"{self.BASE_URL}/api/scrape", json=mixed_payload, timeout=60)
        
        if mixed_response.status_code == 200:
            mixed_data = mixed_response.json()
            print(f"‚úÖ Mixed valid/invalid URLs handled properly")
            print(f"   Processed: {mixed_data.get('processed_count', 0)}")
            print(f"   Failed: {mixed_data.get('failed_count', 0)}")
            
            # Should have at least one success and potentially one failure
            total_processed = mixed_data.get('processed_count', 0) + mixed_data.get('failed_count', 0)
            assert total_processed > 0, "No URLs were processed"
    
    def test_advanced_configuration_options(self):
        """Test advanced configuration options."""
        print("\n‚öôÔ∏è Testing Advanced Configuration Options")
        
        # Test with advanced multi-page configuration
        advanced_payload = {
            "urls": [self.TEST_URL],
            "scraping_mode": "multi",
            "file_format": "text",
            "output_dir": "/tmp",
            "file_mode": "single",
            "multi_page_config": {
                "maxPages": 5,  # Reduced for faster testing
                "crawlDepth": 1,  # Shallow crawl
                "rateLimit": 2000,  # Slower rate limiting
                "includePatterns": "menu,about,contact",
                "excludePatterns": "admin,login,logout,register,cart,checkout,privacy,terms"
            }
        }
        
        response = requests.post(f"{self.BASE_URL}/api/scrape", json=advanced_payload, timeout=90)
        assert response.status_code == 200, "Advanced configuration failed"
        
        data = response.json()
        print(f"‚úÖ Advanced configuration applied successfully")
        print(f"   Max pages configured: 5")
        print(f"   Crawl depth: 1")
        print(f"   Rate limit: 2000ms")
        
        sites_data = data.get("sites_data", [])
        if sites_data:
            total_pages = sum(site.get("pages_processed", 0) for site in sites_data)
            print(f"   Actual pages processed: {total_pages}")
            print(f"   Respected max pages limit: {total_pages <= 5}")
        
        return data
    
    def test_memory_and_performance_monitoring(self):
        """Test memory and performance monitoring."""
        print("\nüîß Testing Memory and Performance Monitoring")
        
        # Check if progress API provides memory information
        progress_response = requests.get(f"{self.BASE_URL}/api/progress")
        if progress_response.status_code == 200:
            progress_data = progress_response.json()
            memory_usage = progress_data.get("memory_usage_mb", 0)
            print(f"‚úÖ Memory monitoring available")
            print(f"   Current memory usage: {memory_usage} MB")
        
        # Test processing time tracking
        start_time = time.time()
        
        payload = {
            "urls": [self.TEST_URL],
            "scraping_mode": "single",
            "file_format": "text",
            "output_dir": "/tmp",
            "file_mode": "single"
        }
        
        response = requests.post(f"{self.BASE_URL}/api/scrape", json=payload, timeout=60)
        
        end_time = time.time()
        request_duration = end_time - start_time
        
        if response.status_code == 200:
            data = response.json()
            reported_time = data.get("processing_time", 0)
            
            print(f"‚úÖ Performance monitoring available")
            print(f"   Request duration: {request_duration:.2f}s")
            print(f"   Reported processing time: {reported_time:.2f}s")
            print(f"   Time tracking accurate: {abs(reported_time - request_duration) < 10}")
    
    def test_complete_workflow_demonstration(self):
        """Demonstrate complete RAG Scraper workflow."""
        print("\nüéØ Complete Workflow Demonstration")
        print("=" * 60)
        
        workflow_steps = [
            "1. Single-page scraping with progress tracking",
            "2. Multi-page scraping with enhanced visualization", 
            "3. Batch processing multiple URLs",
            "4. Real-time progress monitoring",
            "5. File generation and download",
            "6. Error handling and recovery",
            "7. Advanced configuration",
            "8. Performance monitoring"
        ]
        
        print("üöÄ Executing complete RAG Scraper workflow:")
        for step in workflow_steps:
            print(f"   {step}")
        
        print("\n" + "=" * 60)
        
        # Execute all workflow steps
        results = {}
        
        try:
            print("Step 1: Single-page scraping...")
            results["single_page"] = self.test_single_page_scraping_with_progress()
            
            print("\nStep 2: Multi-page scraping...")
            results["multi_page"] = self.test_multi_page_scraping_with_enhanced_progress()
            
            print("\nStep 3: Batch processing...")
            results["batch_processing"] = self.test_batch_processing_multiple_urls()
            
            print("\nStep 4: Progress monitoring...")
            results["progress_api"] = self.test_progress_api_real_time_features()
            
            print("\nStep 5: File generation...")
            results["file_generation"] = self.test_file_generation_and_download()
            
            print("\nStep 6: Error handling...")
            self.test_error_handling_and_recovery()
            
            print("\nStep 7: Advanced configuration...")
            results["advanced_config"] = self.test_advanced_configuration_options()
            
            print("\nStep 8: Performance monitoring...")
            self.test_memory_and_performance_monitoring()
            
        except Exception as e:
            print(f"‚ùå Workflow step failed: {e}")
            raise
        
        # Summary
        print("\n" + "=" * 60)
        print("üéâ COMPLETE WORKFLOW DEMONSTRATION SUCCESSFUL")
        print("=" * 60)
        
        print("\nüìä Workflow Summary:")
        for step_name, result in results.items():
            if isinstance(result, dict):
                processed = result.get("processed_count", 0)
                failed = result.get("failed_count", 0)
                time_taken = result.get("processing_time", 0)
                print(f"   {step_name}: {processed} processed, {failed} failed ({time_taken:.2f}s)")
        
        print("\n‚úÖ All RAG Scraper features demonstrated successfully!")
        print("‚úÖ Progress visualization features working!")
        print("‚úÖ Multi-page scraping operational!")
        print("‚úÖ Real-time monitoring functional!")
        print("‚úÖ Error handling robust!")
        print("‚úÖ File generation working!")
        print("‚úÖ Professional-grade functionality validated!")
        
        return results


if __name__ == "__main__":
    # Run the complete workflow test if executed directly
    test_instance = TestComprehensiveFeatures()
    test_instance.setup_test()
    test_instance.test_complete_workflow_demonstration()